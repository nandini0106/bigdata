# -*- coding: utf-8 -*-
"""SQL_lab_solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i1Zr3t8qJVXNaciYzHmIuCCPJYjlQRgh
"""

# Install Hadoop and Spark on top of it
!apt-get install openjdk-8-jdk-headless -qq > /dev/null ### Installing Java
!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz #### Installing Spark and Hadoop on the worker node
!tar xf spark-3.1.1-bin-hadoop3.2.tgz
!pip install -q findspark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.conf.set("spark.sql.repl.eagerEval.enabled", True) # Property used to format output tables better
spark #### Creating a Sparksession

# Upload the dataset to Google Colab test.csv which describes the customer purchase behaviour
df = spark.read.csv('test.csv', header=True)

# Show Column details of the dataset
df.columns

# Display the first 15 rows of the dataset
df.show(15)

# Print the total number of rows in the dataset
df.count()

# Determine the total Purchase per City Category
df1=df.select('City_Category','Product_Category_1','Product_Category_2','Product_Category_3')

df1.show()

from pyspark.sql.functions import col
 
df2=df1.withColumn("Total", (col("Product_Category_1")+col("Product_Category_2")+col("Product_Category_3")))
df2.show()

import pyspark.sql.functions as fn
means = df2.agg(*[fn.mean(c).alias(c) 
 for c in df2.columns if c != 'City_Category']).toPandas().to_dict('records')[0]
means

df3=df2.fillna(means)

df3.show()

df3.groupBy('City_Category').sum('Total')

# Remove rows which have missing values
len(df.columns)

df.dropna(thresh=len(df.columns)).show()

# Determine the number of users who have age in the age range 46-50 years
df.filter(col('Age')=='46-50').show(truncate=False)

import pyspark.sql.functions as fn
df.agg(
fn.count('User_ID').alias('count'),
fn.countDistinct('User_ID').alias('distinct')
).show()

# Upload daily_weather.csv file
df_weather = spark.read.csv("daily_weather.csv", header=True, inferSchema=True)

# number of duplicate rows

number_of_duplicate_rows = df_weather.count()-df_weather.distinct().count()
number_of_duplicate_rows

# Percentage of missing values
import pyspark.sql.functions as fn
df_weather.agg(*[
 (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing')
 for c in df.columns
]).show()

# Calculate the number of rows which contains atleast one null value.
df_weather.count()-df_weather.dropna(how="any").count()

# Drop all null values and Calculate the mean values for each column.
df_remove_all = df_weather.dropna(how="any")

from pyspark.sql.functions import avg
imputeDF=df_weather
for c in imputeDF.columns:
    meanvalue = df_remove_all.agg(avg(c)).first()[0]
    print(c, meanvalue)
    imputeDF = imputeDF.na.fill(meanvalue, [c])